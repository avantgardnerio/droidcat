same-period hold-out evaluation: work on {'benign': ['zoobenign2010'], 'malware': ['zoo2010']} ... 

loaded from zoobenign2010: 1344 feature vectors; feature vector length: 33152
loaded from zoo2010: 1877 feature vectors; feature vector length: 55256
feature vector length=55256
======== in dataset =======
BENIGN  1344
MALICIOUS   1877
BENIGN  MALICIOUS
model SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',
    max_iter=-1, probability=False, random_state=None, shrinking=True,
      tol=0.001, verbose=False)
2159 samples for training, 1062 samples held out will be used for testing
model LinearSVC(C=0.01, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l1', random_state=None, tol=0.0001,
               verbose=0)
2159 samples for training, 1062 samples held out will be used for testing
model LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=1000, multi_class='ovr', n_jobs=1,
                    penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
                              verbose=0, warm_start=False)
2159 samples for training, 1062 samples held out will be used for testing
precision
0.860016275754  0.859466815868  0.905904313013  
recall
0.854048964218  0.858757062147  0.905838041431  
F1
0.85490403924   0.857419748616  0.905867896859  
accuracy
0.854048964218  0.858757062147  0.905838041431  
same-period hold-out evaluation: work on {'benign': ['zoobenign2011'], 'malware': ['zoo2011']} ... 
loaded from zoobenign2011: 1757 feature vectors; feature vector length: 120867
loaded from zoo2011: 1303 feature vectors; feature vector length: 136655
feature vector length=167693
======== in dataset =======
BENIGN  1757
MALICIOUS   1303
BENIGN  MALICIOUS
model SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',
    max_iter=-1, probability=False, random_state=None, shrinking=True,
      tol=0.001, verbose=False)
2052 samples for training, 1008 samples held out will be used for testing
model LinearSVC(C=0.01, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l1', random_state=None, tol=0.0001,
               verbose=0)
2052 samples for training, 1008 samples held out will be used for testing
model LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=1000, multi_class='ovr', n_jobs=1,
                    penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
                              verbose=0, warm_start=False)
2052 samples for training, 1008 samples held out will be used for testing
precision
0.870036657956  0.865475034454  0.871336400145  
recall
0.864087301587  0.863095238095  0.87003968254   
F1
0.8616243344    0.861473126433  0.868873674041  
accuracy
0.864087301587  0.863095238095  0.87003968254   
same-period hold-out evaluation: work on {'benign': ['zoobenign2012'], 'malware': ['zoo2012']} ... 
loaded from zoobenign2012: 1845 feature vectors; feature vector length: 153096
loaded from zoo2012: 1945 feature vectors; feature vector length: 381728
feature vector length=475390
======== in dataset =======
BENIGN  1845
MALICIOUS   1945
BENIGN  MALICIOUS
model SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',
    max_iter=-1, probability=False, random_state=None, shrinking=True,
      tol=0.001, verbose=False)
2541 samples for training, 1249 samples held out will be used for testing
model LinearSVC(C=0.01, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l1', random_state=None, tol=0.0001,
               verbose=0)
2541 samples for training, 1249 samples held out will be used for testing
model LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=1000, multi_class='ovr', n_jobs=1,
                    penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
                              verbose=0, warm_start=False)
2541 samples for training, 1249 samples held out will be used for testing
precision
0.838260774322  0.890551202641  0.86918422396   
recall
0.838270616493  0.889511609287  0.868694955965  
F1
0.838246333425  0.88951798365   0.868716842496  
accuracy
0.838270616493  0.889511609287  0.868694955965  
same-period hold-out evaluation: work on {'benign': ['zoobenign2013'], 'malware': ['vs2013']} ... 
loaded from zoobenign2013: 1568 feature vectors; feature vector length: 186349
loaded from vs2013: 1139 feature vectors; feature vector length: 186431
feature vector length=584467
======== in dataset =======
BENIGN  1568
MALICIOUS   1139
BENIGN  MALICIOUS
model SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',
    max_iter=-1, probability=False, random_state=None, shrinking=True,
      tol=0.001, verbose=False)
1815 samples for training, 892 samples held out will be used for testing





============================================



hcai@hcai-dl580:~/Downloads/rd_workspace/revealdroid$ time python tab_holdout_revealdroid_all.py 
same-period hold-out evaluation: work on {'benign': ['zoobenign2010'], 'malware': ['zoo2010']} ... 

loaded from zoobenign2010: 1344 feature vectors; feature vector length: 33152
loaded from zoo2010: 1877 feature vectors; feature vector length: 55256
feature vector length=55256
======== in dataset =======
BENIGN  1344
MALICIOUS   1877
BENIGN  MALICIOUS
model SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',
    max_iter=-1, probability=False, random_state=None, shrinking=True,
      tol=0.001, verbose=False)
2159 samples for training, 1062 samples held out will be used for testing
model LinearSVC(C=0.01, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l1', random_state=None, tol=0.0001,
               verbose=0)
2159 samples for training, 1062 samples held out will be used for testing
model LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=1000, multi_class='ovr', n_jobs=1,
                    penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
                              verbose=0, warm_start=False)
2159 samples for training, 1062 samples held out will be used for testing
precision
0.860016275754  0.859466815868  0.905904313013  
recall
0.854048964218  0.858757062147  0.905838041431  
F1
0.85490403924   0.857419748616  0.905867896859  
accuracy
0.854048964218  0.858757062147  0.905838041431  
same-period hold-out evaluation: work on {'benign': ['zoobenign2011'], 'malware': ['zoo2011']} ... 
loaded from zoobenign2011: 1757 feature vectors; feature vector length: 120867
loaded from zoo2011: 1303 feature vectors; feature vector length: 136655
feature vector length=167693
======== in dataset =======
BENIGN  1757
MALICIOUS   1303
BENIGN  MALICIOUS
model SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',
    max_iter=-1, probability=False, random_state=None, shrinking=True,
      tol=0.001, verbose=False)
2052 samples for training, 1008 samples held out will be used for testing
model LinearSVC(C=0.01, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l1', random_state=None, tol=0.0001,
               verbose=0)
2052 samples for training, 1008 samples held out will be used for testing
model LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=1000, multi_class='ovr', n_jobs=1,
                    penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
                              verbose=0, warm_start=False)
2052 samples for training, 1008 samples held out will be used for testing
precision
0.870036657956  0.865475034454  0.871336400145  
recall
0.864087301587  0.863095238095  0.87003968254   
F1
0.8616243344    0.861473126433  0.868873674041  
accuracy
0.864087301587  0.863095238095  0.87003968254   
same-period hold-out evaluation: work on {'benign': ['zoobenign2012'], 'malware': ['zoo2012']} ... 
loaded from zoobenign2012: 1845 feature vectors; feature vector length: 153096
loaded from zoo2012: 1945 feature vectors; feature vector length: 381728
feature vector length=475390
======== in dataset =======
BENIGN  1845
MALICIOUS   1945
BENIGN  MALICIOUS
model SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',
    max_iter=-1, probability=False, random_state=None, shrinking=True,
      tol=0.001, verbose=False)
2541 samples for training, 1249 samples held out will be used for testing
model LinearSVC(C=0.01, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l1', random_state=None, tol=0.0001,
               verbose=0)
2541 samples for training, 1249 samples held out will be used for testing
model LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=1000, multi_class='ovr', n_jobs=1,
                    penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
                              verbose=0, warm_start=False)
2541 samples for training, 1249 samples held out will be used for testing
precision
0.838260774322  0.890551202641  0.86918422396   
recall
0.838270616493  0.889511609287  0.868694955965  
F1
0.838246333425  0.88951798365   0.868716842496  
accuracy
0.838270616493  0.889511609287  0.868694955965  
same-period hold-out evaluation: work on {'benign': ['zoobenign2013'], 'malware': ['vs2013']} ... 
loaded from zoobenign2013: 1568 feature vectors; feature vector length: 186349
loaded from vs2013: 1139 feature vectors; feature vector length: 186431
feature vector length=584467
======== in dataset =======
BENIGN  1568
MALICIOUS   1139
BENIGN  MALICIOUS
model SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',
    max_iter=-1, probability=False, random_state=None, shrinking=True,
      tol=0.001, verbose=False)
1815 samples for training, 892 samples held out will be used for testing



model LinearSVC(C=0.01, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l1', random_state=None, tol=0.0001,
               verbose=0)
1815 samples for training, 892 samples held out will be used for testing
model LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=1000, multi_class='ovr', n_jobs=1,
                    penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
                              verbose=0, warm_start=False)
1815 samples for training, 892 samples held out will be used for testing
precision
0.880128370501  0.943027411578  0.940541775066  
recall
0.872197309417  0.942825112108  0.940582959641  
F1
0.872968096281  0.942650476256  0.940502836904  
accuracy
0.872197309417  0.942825112108  0.940582959641  
same-period hold-out evaluation: work on {'benign': ['zoobenign2014'], 'malware': ['vs2014']} ... 
loaded from zoobenign2014: 2953 feature vectors; feature vector length: 416017
loaded from vs2014: 1337 feature vectors; feature vector length: 495422










feature vector length=912415

======== in dataset =======
BENIGN  2953
MALICIOUS   1337
BENIGN  MALICIOUS
model SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',
    max_iter=-1, probability=False, random_state=None, shrinking=True,
      tol=0.001, verbose=False)
2875 samples for training, 1415 samples held out will be used for testing
^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^\^\^\^C^C^C^C^C^CQuit (core dumped)

real    1512m0.228s
user    825m21.528s
sys 198m41.984s



==============================
























hcai@hcai-dl580:~/Downloads/rd_workspace/revealdroid$ time python tab_holdout_revealdroid_all.py 
same-period hold-out evaluation: work on {'benign': ['zoobenign2011'], 'malware': ['zoo2011']} ... 
loaded from zoobenign2011: 1757 feature vectors; feature vector length: 120867
loaded from zoo2011: 1303 feature vectors; feature vector length: 136655

feature vector length=136655
======== in dataset =======
BENIGN  1757
MALICIOUS   1303
BENIGN  MALICIOUS
model SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',
    max_iter=-1, probability=False, random_state=None, shrinking=True,
      tol=0.001, verbose=False)
2052 samples for training, 1008 samples held out will be used for testing
model LinearSVC(C=0.01, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l1', random_state=None, tol=0.0001,
               verbose=0)
2052 samples for training, 1008 samples held out will be used for testing
model LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=1000, multi_class='ovr', n_jobs=1,
                    penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
                              verbose=0, warm_start=False)
2052 samples for training, 1008 samples held out will be used for testing
precision
0.871485052405  0.871485052405  0.873826348183  
recall
0.863095238095  0.863095238095  0.874007936508  
F1
0.86010662692   0.86010662692   0.873598256217  
accuracy
0.863095238095  0.863095238095  0.874007936508  
same-period hold-out evaluation: work on {'benign': ['zoobenign2012'], 'malware': ['zoo2012']} ... 
loaded from zoobenign2012: 1845 feature vectors; feature vector length: 153096
loaded from zoo2012: 1945 feature vectors; feature vector length: 381728
feature vector length=381728
======== in dataset =======
BENIGN  1845
MALICIOUS   1945
BENIGN  MALICIOUS
model SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',
    max_iter=-1, probability=False, random_state=None, shrinking=True,
      tol=0.001, verbose=False)
2541 samples for training, 1249 samples held out will be used for testing
model LinearSVC(C=0.01, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l1', random_state=None, tol=0.0001,
               verbose=0)
2541 samples for training, 1249 samples held out will be used for testing
model LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=1000, multi_class='ovr', n_jobs=1,
                    penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
                              verbose=0, warm_start=False)
2541 samples for training, 1249 samples held out will be used for testing
precision
0.828280534906  0.893379929792  0.899114849009  
recall
0.827862289832  0.8903122498    0.899119295436  
F1
0.827683118082  0.890237699312  0.899114765184  
accuracy
0.827862289832  0.8903122498    0.899119295436  
same-period hold-out evaluation: work on {'benign': ['zoobenign2013'], 'malware': ['vs2013']} ... 
loaded from zoobenign2013: 1568 feature vectors; feature vector length: 186349
loaded from vs2013: 1139 feature vectors; feature vector length: 186431
feature vector length=186431
======== in dataset =======
BENIGN  1568
MALICIOUS   1139
BENIGN  MALICIOUS
model SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',
    max_iter=-1, probability=False, random_state=None, shrinking=True,
      tol=0.001, verbose=False)
1815 samples for training, 892 samples held out will be used for testing
model LinearSVC(C=0.01, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l1', random_state=None, tol=0.0001,
               verbose=0)
1815 samples for training, 892 samples held out will be used for testing
model LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=1000, multi_class='ovr', n_jobs=1,
                    penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
                              verbose=0, warm_start=False)
1815 samples for training, 892 samples held out will be used for testing
precision
0.891462537379  0.906924465934  0.944207353105  
recall
0.882286995516  0.902466367713  0.943946188341  
F1
0.88301780035   0.902965431601  0.943762324275  
accuracy
0.882286995516  0.902466367713  0.943946188341  
same-period hold-out evaluation: work on {'benign': ['zoobenign2014'], 'malware': ['vs2014']} ... 
loaded from zoobenign2014: 2953 feature vectors; feature vector length: 416017
loaded from vs2014: 1337 feature vectors; feature vector length: 495422
feature vector length=495422
======== in dataset =======
BENIGN  2953
MALICIOUS   1337
BENIGN  MALICIOUS
model SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',
    max_iter=-1, probability=False, random_state=None, shrinking=True,
      tol=0.001, verbose=False)
2875 samples for training, 1415 samples held out will be used for testing





























model LinearSVC(C=0.01, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l1', random_state=None, tol=0.0001,
               verbose=0)
2875 samples for training, 1415 samples held out will be used for testing
model LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=1000, multi_class='ovr', n_jobs=1,
                    penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
                              verbose=0, warm_start=False)
2875 samples for training, 1415 samples held out will be used for testing
precision
0.835978401026  0.85900044583   0.839205650479  
recall
0.838869257951  0.860070671378  0.833922261484  
F1
0.836662958032  0.859441447226  0.835825564728  
accuracy
0.838869257951  0.860070671378  0.833922261484  



same-period hold-out evaluation: work on {'benign': ['zoobenign2015'], 'malware': ['vs2015']} ... 
loaded from zoobenign2015: 1178 feature vectors; feature vector length: 272064
loaded from vs2015: 1451 feature vectors; feature vector length: 630199




feature vector length=630199
======== in dataset =======
BENIGN  1178
MALICIOUS   1451
BENIGN  MALICIOUS
model SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',
    max_iter=-1, probability=False, random_state=None, shrinking=True,
      tol=0.001, verbose=False)
1763 samples for training, 866 samples held out will be used for testing












model LinearSVC(C=0.01, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l1', random_state=None, tol=0.0001,
               verbose=0)
1763 samples for training, 866 samples held out will be used for testing
model LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=1000, multi_class='ovr', n_jobs=1,
                    penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
                              verbose=0, warm_start=False)
1763 samples for training, 866 samples held out will be used for testing
precision
0.823570811272  0.843894503434  0.845445217582  
recall
0.801385681293  0.841801385681  0.845265588915  
F1
0.793900976736  0.842146111525  0.845337271523  
accuracy
0.801385681293  0.841801385681  0.845265588915  
same-period hold-out evaluation: work on {'benign': ['zoobenign2016'], 'malware': ['vs2016']} ... 
loaded from zoobenign2016: 1370 feature vectors; feature vector length: 375814
loaded from vs2016: 1769 feature vectors; feature vector length: 758965
feature vector length=758965
======== in dataset =======
BENIGN  1370
MALICIOUS   1769
BENIGN  MALICIOUS
model SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',
    max_iter=-1, probability=False, random_state=None, shrinking=True,
      tol=0.001, verbose=False)
2104 samples for training, 1035 samples held out will be used for testing
model LinearSVC(C=0.01, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l1', random_state=None, tol=0.0001,
               verbose=0)
2104 samples for training, 1035 samples held out will be used for testing
model LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=1000, multi_class='ovr', n_jobs=1,
                    penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
                              verbose=0, warm_start=False)
2104 samples for training, 1035 samples held out will be used for testing
precision
0.865981662268  0.903006744206  0.839923602474  
recall
0.844444444444  0.902415458937  0.836714975845  
F1
0.83886106305   0.901982423891  0.83724953081   
accuracy
0.844444444444  0.902415458937  0.836714975845  
same-period hold-out evaluation: work on {'benign': ['benign2017'], 'malware': ['zoo2017']} ... 
loaded from benign2017: 1612 feature vectors; feature vector length: 426480
loaded from zoo2017: 148 feature vectors; feature vector length: 438784



feature vector length=438784
======== in dataset =======
BENIGN  1612
MALICIOUS   148
BENIGN  MALICIOUS
model SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',
    max_iter=-1, probability=False, random_state=None, shrinking=True,
      tol=0.001, verbose=False)
1181 samples for training, 579 samples held out will be used for testing





model LinearSVC(C=0.01, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
          multi_class='ovr', penalty='l1', random_state=None, tol=0.0001,
               verbose=0)
1181 samples for training, 579 samples held out will be used for testing
model LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=1000, multi_class='ovr', n_jobs=1,
                    penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
                              verbose=0, warm_start=False)
1181 samples for training, 579 samples held out will be used for testing
precision
0.9469744883    0.933843586816  0.934599346097  
recall
0.946459412781  0.92573402418   0.932642487047  
F1
0.946710163998  0.929162792033  0.933567648183  
accuracy
0.946459412781  0.92573402418   0.932642487047  

real    2252m32.066s
user    1359m56.088s
sys 281m32.996s


